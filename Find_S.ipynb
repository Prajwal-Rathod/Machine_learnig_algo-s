{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sunny' 'warm' 'normal' 'strong' 'warm' 'same']\n",
      " ['sunny' 'warm' 'high' 'strong' 'warm' 'same']\n",
      " ['rainy' 'cold' 'high' 'strong' 'warm' 'change']\n",
      " ['sunny' 'warm' 'high' 'strong' 'cool' 'change']]\n",
      "['yes' 'yes' 'no' 'yes']\n",
      "Initial specific hypothesis (first 'yes' instance): ['sunny' 'warm' 'normal' 'strong' 'warm' 'same']\n",
      "\n",
      "Considering instance 1: ['sunny' 'warm' 'normal' 'strong' 'warm' 'same']\n",
      "Updated specific hypothesis: ['sunny' 'warm' 'normal' 'strong' 'warm' 'same']\n",
      "\n",
      "Considering instance 2: ['sunny' 'warm' 'high' 'strong' 'warm' 'same']\n",
      "Updated specific hypothesis: ['sunny' 'warm' '?' 'strong' 'warm' 'same']\n",
      "\n",
      "Considering instance 4: ['sunny' 'warm' 'high' 'strong' 'cool' 'change']\n",
      "Updated specific hypothesis: ['sunny' 'warm' '?' 'strong' '?' '?']\n",
      "\n",
      "Final specific hypothesis: ['sunny' 'warm' '?' 'strong' '?' '?']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(\"Finds.csv\")\n",
    "attribute = np.array(data)[:, :-1]\n",
    "target = np.array(data)[:, -1]\n",
    "print(attribute)\n",
    "print(target)\n",
    "\n",
    "def train(att, tar):\n",
    "    specific_h = None\n",
    "\n",
    "    for i, val in enumerate(tar):\n",
    "        if val == 'yes':\n",
    "            specific_h = att[i].copy()\n",
    "            print(f\"Initial specific hypothesis (first 'yes' instance): {specific_h}\")\n",
    "            break\n",
    "\n",
    "    for i, val in enumerate(att):\n",
    "        if tar[i] == 'yes':\n",
    "            print(f\"\\nConsidering instance {i + 1}: {val}\")\n",
    "            for x in range(len(specific_h)):\n",
    "                if val[x] != specific_h[x]:\n",
    "                    specific_h[x] = '?'\n",
    "            print(f\"Updated specific hypothesis: {specific_h}\")\n",
    "\n",
    "    return specific_h\n",
    "\n",
    "specific_hypothesis = train(attribute, target)\n",
    "print(\"\\nFinal specific hypothesis:\", specific_hypothesis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate elimination algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration[1]\n",
      "Specific: ['sunny' 'warm' 'normal' 'strong' 'warm' 'same']\n",
      "General: [['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n",
      "\n",
      "\n",
      "Iteration[2]\n",
      "Specific: ['sunny' 'warm' '?' 'strong' 'warm' 'same']\n",
      "General: [['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n",
      "\n",
      "\n",
      "Iteration[3]\n",
      "Specific: ['sunny' 'warm' '?' 'strong' 'warm' 'same']\n",
      "General: [['sunny', '?', '?', '?', '?', '?'], ['?', 'warm', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', 'same']]\n",
      "\n",
      "\n",
      "Iteration[4]\n",
      "Specific: ['sunny' 'warm' '?' 'strong' '?' '?']\n",
      "General: [['sunny', '?', '?', '?', '?', '?'], ['?', 'warm', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?'], ['?', '?', '?', '?', '?', '?']]\n",
      "\n",
      "\n",
      "Final hypothesis :-\n",
      "Specific hypothesis:['sunny' 'warm' '?' 'strong' '?' '?']\n",
      "General hypothesis:[['sunny', '?', '?', '?', '?', '?'], ['?', 'warm', '?', '?', '?', '?']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"Finds.csv\")\n",
    "\n",
    "concept = np.array(data)[:,:-1]\n",
    "target = np.array(data)[:,-1]\n",
    "\n",
    "def train(con, tar):\n",
    "    specific_h = con[0].copy()\n",
    "    general_h = [['?' for _ in range(len(specific_h))] for _ in range(len(specific_h))]\n",
    "    \n",
    "    for i, val in enumerate(con):\n",
    "        if tar[i] == 'yes':\n",
    "            for x in range(len(specific_h)):\n",
    "                if val[x]!= specific_h[x]:\n",
    "                    specific_h[x] = '?'\n",
    "                    general_h[x][x] = '?'\n",
    "        else:\n",
    "            for x in range(len(specific_h)):\n",
    "                if val[x]!= specific_h[x]:\n",
    "                    general_h[x][x] = specific_h[x]\n",
    "                else:\n",
    "                    general_h[x][x] = '?'\n",
    "        \n",
    "        print(\"Iteration[\"+ str(i+1) +\"]\")\n",
    "        print(\"Specific: \"+str(specific_h))\n",
    "        print(\"General: \"+str(general_h)+\"\\n\\n\")\n",
    "        \n",
    "    general_h = [general_h[i] for i, val in enumerate(general_h) if val!= ['?' for _ in range(len(specific_h))]]\n",
    "    return specific_h, general_h  \n",
    "\n",
    "specific, general = train(concept, target)\n",
    "\n",
    "print(\"Final hypothesis :-\")\n",
    "print(\"Specific hypothesis:\" +str(specific))\n",
    "print(\"General hypothesis:\" +str(general))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [True, True, True],  # Fever, Chills, Headache\n",
    "    [True, True, False],\n",
    "    [True, False, True],\n",
    "    [False, True, False],\n",
    "    [False, False, False],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyHSmm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyHSmm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesianNetwork\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Define the variables\u001b[39;00m\n\u001b[0;32m      4\u001b[0m variables \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFever\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChills\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeadache\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyHSmm'"
     ]
    }
   ],
   "source": [
    "from pyHSmm import BayesianNetwork\n",
    "\n",
    "# Define the variables\n",
    "variables = [\"Fever\", \"Chills\", \"Headache\"]\n",
    "\n",
    "# Define the conditional probability tables (CPTs)\n",
    "fever_cpt = [[0.9, 0.1], [0.2, 0.8]]  # P(Fever | No Parents)\n",
    "chills_cpt = [[0.7, 0.3], [0.2, 0.8]]  # P(Chills | Fever)\n",
    "headache_cpt = [[0.8, 0.2], [0.5, 0.5]]  # P(Headache | Fever)\n",
    "\n",
    "# Structure: Fever has no parents, Chills depends on Fever, Headache depends on Fever\n",
    "network = BayesianNetwork(variables)\n",
    "network.add_cpts(fever_cpt, chills_cpt, headache_cpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network.probability(\"Fever\"))  # Output: 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network.probability(\"Fever\", evidence={\"Chills\": True}))  # Output: 0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9504\n",
      "Classification Report:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       1.00      0.88      0.93       252\n",
      "         comp.graphics       0.97      0.99      0.98       295\n",
      "               sci.med       0.99      0.94      0.97       299\n",
      "soc.religion.christian       0.87      0.98      0.92       282\n",
      "\n",
      "              accuracy                           0.95      1128\n",
      "             macro avg       0.96      0.95      0.95      1128\n",
      "          weighted avg       0.95      0.95      0.95      1128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load Data\n",
    "# Fetch the 20 newsgroups dataset\n",
    "categories = ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
    "data = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Extract features and labels\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert text data into numerical features using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize and train the Na√Øve Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "\n",
    "# Compute accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=data.target_names)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.distrib import Distribution\n",
    "\n",
    "# Define the structure of the Bayesian Network\n",
    "model = BayesianModel([('Fever', 'Disease'), ('Chills', 'Disease'), ('Headache', 'Disease')])\n",
    "\n",
    "# Define the data\n",
    "data = [\n",
    "    [True, True, True],  # Fever, Chills, Headache\n",
    "    [True, True, False],\n",
    "    [True, False, True],\n",
    "    [False, True, False],\n",
    "    [False, False, False],\n",
    "]\n",
    "\n",
    "# Group the data by the first two symptoms\n",
    "grouped_data = defaultdict(list)\n",
    "for row in data:\n",
    "    grouped_data[(row[0], row[1])].append(row[2])\n",
    "\n",
    "# Convert the grouped data into a contingency table\n",
    "contingency_table = {}\n",
    "for symptoms, values in grouped_data.items():\n",
    "    contingency_table[(symptoms, True)] = sum(values)\n",
    "    contingency_table[(symptoms, False)] = len(data) - sum(values)\n",
    "\n",
    "# Define the conditional probability distributions\n",
    "dist_fever = Distribution({'Fever': {'yes': contingency_table[((True,), True)], 'no': contingency_table[((True,), False)]}})\n",
    "dist_chills = Distribution({'Chills': {'yes': contingency_table[((True, True), True)] + contingency_table[((False, True), True)], 'no': contingency_table[((True, True), False)] + contingency_table[((False, True), False)]}})\n",
    "dist_headache = Distribution({'Headache': {'yes': contingency_table[((True, True), True)] + contingency_table[((True, False), True)], 'no': contingency_table[((True, True), False)] + contingency_table[((True, False), False)]}})\n",
    "dist_disease = Distribution({'Disease': {'yes': contingency_table[((True, True), True)] + contingency_table[((True, False), True)] + contingency_table[((False, True), True)], 'no': contingency_table[((False, False), False)]}})\n",
    "\n",
    "\n",
    "# Add the distributions to the model\n",
    "model.add_cpds(dist_fever, variable='Fever')\n",
    "model.add_cpds(dist_chills, variable='Chills')\n",
    "model.add_cpds(dist_headache, variable='Headache')\n",
    "model.add_cpds(dist_disease, variable='Disease')\n",
    "\n",
    "# Query the network\n",
    "print(model.predict('Disease', evidence={'Fever': 'yes', 'Chills': 'yes', 'Headache': 'yes'}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
